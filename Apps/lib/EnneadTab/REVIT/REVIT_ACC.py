_api_help_website = "https://aps.autodesk.com/en/docs/bim360/v1/reference/http/"

"""
Key functions and their usage:

COST-SAVING OPTIMIZATIONS:
- Token reuse across multiple projects (saves 80%+ token calls)
- Smart project filtering to skip empty projects 
- Aggressive caching with 2-week expiry
- Early exit for projects without Revit files
- API call monitoring and cost tracking

DATA GENERATION PIPELINE:
1. PROJECT DETAILS (get_acc_projects_data):
   - Raw hub and project data from Autodesk API
   - Structure: {hub_name: {data: [project_objects]}}
   - Cached as: ACC_PROJECTS_CACHED_DETAILS
   - Generated by: get_all_acc_projects_data_action()
   - Contains: Basic project metadata, IDs, names, types
   
2. PROJECT SUMMARY (get_ACC_summary_data):
   - Enhanced project data with Revit file details
   - Structure: {project_name: {project_id, project_type, revit_files: {file_name: file_details}}}
   - Cached as: ACC_PROJECTS_CACHED_SUMMARY
   - Generated by: get_ACC_summary_data() -> calls get_project_revit_files_data() for each project
   - Contains: Complete project + Revit file information with metadata
   
3. PROJECT BY YEAR (generated alongside summary):
   - Projects grouped by Revit version years
   - Structure: {version_year_list: [project_names]}
   - Cached as: ACC_PROJECTS_CACHED_BY_YEAR
   - Used for: Version-specific project filtering

REGENERATION TRIGGERS:
- Cache older than CACHE_EXPIRY_DAYS (14 days)
- Missing cache files
- Debug mode enabled
- Manual cache clearing

"""

# ============================================================================
# GLOBAL FILE NAMING CONSTANTS
# ============================================================================
# Cache file naming templates - explicitly named as cache files
CACHE_ACC_PROJECTS_DETAILS = "ACC_PROJECTS_CACHED_DETAILS"
CACHE_ACC_PROJECTS_SUMMARY = "ACC_PROJECTS_CACHED_SUMMARY" 
CACHE_ACC_PROJECTS_BY_YEAR = "ACC_PROJECTS_CACHED_BY_YEAR"
CACHE_ACC_PROJECT_REVIT_FILES_TEMPLATE = "ACC_PROJECT_CACHED_REVIT_FILES_DETAIL_{project_name}_{hub_name}"
CACHE_ACC_PROJECT_REVIT_FILES_ID_TEMPLATE = "ACC_PROJECT_CACHED_REVIT_FILES_DETAIL_{project_id}_{hub_id}"

# Job and task file naming templates
JOB_FILE_TEMPLATE = "ACC_JOB_{job_id}"
TASK_RUNNER_FILE = "ACC_PROJECT_TASK_RUNNER"

# Log file naming
LOG_FILE_NAME = "EnneadTab_ACC.log"

# Cache expiry settings
CACHE_EXPIRY_DAYS = 14  # 2 weeks
TOKEN_BUFFER_MINUTES = 5  # Token refresh buffer

# API call limits for cost control
MAX_REVIT_FILES_DETAILED = 50  # Limit detailed file info calls
MAX_REVIT_FILES_SEARCH = 20    # Stop search after finding this many files
MAX_FOLDER_DEPTH = 10          # Prevent infinite recursion

# Global API call counter for cost monitoring
API_CALL_COUNTER = {
    "token_calls": 0,
    "project_calls": 0, 
    "folder_calls": 0,
    "file_detail_calls": 0,
    "total_calls": 0,
    "session_start": None
}

def log_api_call(call_type):
    """Log an API call for cost monitoring."""
    global API_CALL_COUNTER
    if API_CALL_COUNTER["session_start"] is None:
        API_CALL_COUNTER["session_start"] = time.time()
    
    API_CALL_COUNTER[call_type] += 1
    API_CALL_COUNTER["total_calls"] += 1
    
    # Log every 10 calls to monitor progress
    if API_CALL_COUNTER["total_calls"] % 10 == 0:
        elapsed = time.time() - API_CALL_COUNTER["session_start"]
        logging.info("API Usage - Total: {}, Token: {}, Project: {}, Folder: {}, FileDetail: {} (in {:.1f}s)".format(
            API_CALL_COUNTER["total_calls"],
            API_CALL_COUNTER["token_calls"], 
            API_CALL_COUNTER["project_calls"],
            API_CALL_COUNTER["folder_calls"],
            API_CALL_COUNTER["file_detail_calls"],
            elapsed
        ))

def get_api_usage_summary():
    """Get a summary of API usage for cost analysis."""
    global API_CALL_COUNTER
    elapsed = time.time() - API_CALL_COUNTER["session_start"] if API_CALL_COUNTER["session_start"] else 0
    
    return {
        "total_calls": API_CALL_COUNTER["total_calls"],
        "token_calls": API_CALL_COUNTER["token_calls"],
        "project_calls": API_CALL_COUNTER["project_calls"], 
        "folder_calls": API_CALL_COUNTER["folder_calls"],
        "file_detail_calls": API_CALL_COUNTER["file_detail_calls"],
        "session_duration": elapsed,
        "calls_per_minute": (API_CALL_COUNTER["total_calls"] / (elapsed / 60)) if elapsed > 0 else 0
    }


import os
import time
import base64
import sys
import threading
import subprocess
import logging
import uuid  # added at top for job id generation
# Setup imports
root_folder = os.path.abspath((os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.append(root_folder)
import SECRET, DATA_FILE, NOTIFICATION, FOLDER

try:
    import requests # pyright: ignore
except ImportError:
    NOTIFICATION.messenger("requests module not found, please install it.")

# Setup logging
logging.basicConfig(
    filename=os.path.join(os.path.expanduser("~"), "Desktop", LOG_FILE_NAME),
    level=logging.DEBUG,
    format='%(asctime)s %(levelname)s %(message)s'
)

def EXAMPLE_1():
    import pprint
    time_start = time.time()
    sample_project = get_project_data_by_name("2317_NYU Kimmel Garage Infill")
    if sample_project:
        project_id = sample_project["id"]
        hub_id = sample_project["relationships"]["hub"]["data"]["id"]
        pprint.pprint(get_project_revit_files_data(project_id, hub_id))

    time_end = time.time()
    print("\nTime taken: {} seconds".format(time_end - time_start))

    time_start = time.time()
    pprint.pprint(get_ACC_summary_data(show_progress=True))
    time_end = time.time()
    print("\nTime taken: {} seconds".format(time_end - time_start))

def run_example_1_in_thread():
    thread = threading.Thread(target=EXAMPLE_1)
    thread.start()
    thread.join()

def EXAMPLE_2_IMPROVED():
    print("Starting EXAMPLE_2_IMPROVED...")
    export_by_project_name_improved("2317_NYU Kimmel Garage Infill")
    print("EXAMPLE_2_IMPROVED completed.")

def export_by_project_name_improved(project_name):
    """Export all sheets and DWGs from BIM360 files in a given project with improved error handling.
    Args:
        project_name (str): The name of the project to export from.
    Returns:
        bool: True if export was successful, False otherwise.
    """
    print("Starting export for project: {}".format(project_name))
    print("Getting project data...")
    project_data = get_project_data_by_name(project_name)
    if not project_data:
        print("Project not found: {}".format(project_name))
        NOTIFICATION.messenger("Project not found: {}".format(project_name))
        return False
    print("Project data found. Project ID: {}".format(project_data["id"]))
    project_id = project_data["id"]
    hub_id = project_data["relationships"]["hub"]["data"]["id"]
    print("Getting Revit files data...")
    revit_files = get_project_revit_files_data(project_id, hub_id)
    if not revit_files:
        print("No Revit files found in project")
        NOTIFICATION.messenger("No Revit files found in project")
        return False
    print("Found {} Revit files".format(len(revit_files)))
    desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
    output_folder = os.path.join(desktop_path, "cloud export")
    if not os.path.exists(output_folder):
        print("Creating output folder: {}".format(output_folder))
        os.makedirs(output_folder)
    print("Getting access token...")
    data = SECRET.get_acc_key_data()
    if not data:
        print("Failed to get ACC key data")
        return False
    client_id = data.get("client_id")
    client_secret = data.get("client_secret")
    if not (client_id and client_secret):
        print("Missing client ID or secret")
        return False
    token_url = "https://developer.api.autodesk.com/authentication/v2/token"
    token_headers = {"Content-Type": "application/x-www-form-urlencoded"}
    token_data = {
        "client_id": client_id,
        "client_secret": client_secret,
        "grant_type": "client_credentials",
        "scope": "data:read data:write data:create data:search bucket:read bucket:create account:read"
    }
    print("Requesting access token...")
    token_resp = requests.post(token_url, headers=token_headers, data=token_data)
    print("Token response status: {}".format(token_resp.status_code))
    print("Token response body: {}".format(token_resp.text))
    if token_resp.status_code != 200:
        print("Failed to get access token. Status code: {}".format(token_resp.status_code))
        return False
    access_token = token_resp.json().get("access_token")
    print("Access token obtained successfully")
    for i, file_data in enumerate(revit_files, 1):
        print("\nProcessing file {}/{}".format(i, len(revit_files)))
        if "data" not in file_data:
            print("Skipping file - no data found")
            continue
        file_id = file_data["data"]["id"]
        file_name = file_data["data"]["attributes"]["displayName"]
        print("Processing file: {}".format(file_name))
        versions_url = "https://developer.api.autodesk.com/data/v1/projects/{}/items/{}/versions".format(project_id, file_id)
        versions_headers = {"Authorization": "Bearer {}".format(access_token)}
        print("Getting file versions...")
        versions_resp = requests.get(versions_url, headers=versions_headers)
        print("Versions response status: {}".format(versions_resp.status_code))
        print("Versions response body: {}".format(versions_resp.text))
        if versions_resp.status_code != 200:
            print("Failed to get versions. Status code: {}".format(versions_resp.status_code))
            continue
        versions_data = versions_resp.json()
        if not versions_data.get("data"):
            print("No versions found")
            continue
        latest_version = versions_data["data"][0]
        urn = latest_version["id"]
        urn_b64 = base64.b64encode(urn.encode("utf-8")).decode("utf-8").rstrip("=")
        print("Using base64 URN: {}".format(urn_b64))
        export_url = "https://developer.api.autodesk.com/modelderivative/v2/designdata/job"
        export_headers = {
            "Authorization": "Bearer {}".format(access_token),
            "Content-Type": "application/json",
            "x-ads-force": "true"
        }
        export_data = {
            "input": {
                "urn": urn_b64
            },
            "output": {
                "formats": [
                    {
                        "type": "dwg",
                        "views": ["2d", "3d"]
                    },
                    {
                        "type": "pdf",
                        "views": ["2d"]
                    }
                ]
            }
        }
        print("Creating export job...")
        export_resp = requests.post(export_url, headers=export_headers, json=export_data)
        print("Export job response status: {}".format(export_resp.status_code))
        print("Export job response body: {}".format(export_resp.text))
        if export_resp.status_code == 409:
            print("Export job already in progress, skipping...")
            continue
        if export_resp.status_code == 406:
            print("File is a shallow copy, skipping...")
            continue
        if export_resp.status_code not in [200, 202]:
            print("Failed to create export job. Status code: {}".format(export_resp.status_code))
            continue
        job_id = urn_b64
        manifest_url = "https://developer.api.autodesk.com/modelderivative/v2/designdata/{}/manifest".format(urn_b64)
        manifest_headers = {"Authorization": "Bearer {}".format(access_token)}
        print("Waiting for export to complete...")
        max_retries = 10
        retry_count = 0
        while retry_count < max_retries:
            manifest_resp = requests.get(manifest_url, headers=manifest_headers)
            print("Manifest response status: {}".format(manifest_resp.status_code))
            print("Manifest response body: {}".format(manifest_resp.text))
            if manifest_resp.status_code == 404:
                print("Manifest not found, retrying... ({}/{})".format(retry_count + 1, max_retries))
                time.sleep(10)
                retry_count += 1
                continue
            if manifest_resp.status_code != 200:
                print("Failed to get manifest. Status code: {}".format(manifest_resp.status_code))
                break
            manifest_data = manifest_resp.json()
            if manifest_data["status"] == "success":
                print("Export successful, downloading files...")
                for derivative in manifest_data.get("derivatives", []):
                    if derivative["type"] in ["dwg", "pdf"]:
                        for output in derivative.get("output", []):
                            download_url = output["url"]
                            download_resp = requests.get(download_url, headers=manifest_headers)
                            if download_resp.status_code == 200:
                                output_file = os.path.join(output_folder, "{}_{}.{}".format(
                                    file_name, output["name"], derivative["type"]))
                                print("Saving file: {}".format(output_file))
                                with open(output_file, "wb") as f:
                                    f.write(download_resp.content)
                break
            elif manifest_data["status"] == "failed":
                print("Export failed")
                break
            print("Export in progress...")
            time.sleep(5)
    print("\nExport completed. Files saved to: {}".format(output_folder))
    NOTIFICATION.messenger("Export completed. Files saved to: {}".format(output_folder))
    return True

# Global token cache to reuse tokens across projects
_CACHED_TOKEN = {
    "access_token": None,
    "expires_at": 0,
    "client_id": None,
    "client_secret": None
}

def get_reusable_access_token():
    """Get a reusable access token that can be shared across multiple API calls.
    
    This dramatically reduces API costs by reusing tokens instead of getting new ones
    for every project. Tokens are valid for 1 hour typically.
    
    Returns:
        str: Access token or None if failed
    """
    global _CACHED_TOKEN
    
    # Check if we have a valid cached token
    current_time = time.time()
    if (_CACHED_TOKEN["access_token"] and 
        current_time < _CACHED_TOKEN["expires_at"] - (TOKEN_BUFFER_MINUTES * 60)):  # Token refresh buffer
        logging.info("Reusing cached access token (expires in {:.1f} minutes)".format(
            (_CACHED_TOKEN["expires_at"] - current_time) / 60))
        return _CACHED_TOKEN["access_token"]
    
    # Need to get a new token
    data = SECRET.get_acc_key_data()
    if not data:
        return None
    
    client_id = data.get("client_id")
    client_secret = data.get("client_secret")
    if not (client_id and client_secret):
        return None

    token_url = "https://developer.api.autodesk.com/authentication/v2/token"
    token_headers = {"Content-Type": "application/x-www-form-urlencoded"}
    token_data = {
        "client_id": client_id,
        "client_secret": client_secret,
        "grant_type": "client_credentials",
        "scope": "data:read data:write data:create data:search bucket:read bucket:create account:read"
    }
    
    try:
        log_api_call("token_calls")
        token_resp = requests.post(token_url, headers=token_headers, data=token_data, timeout=30)
        if token_resp.status_code != 200:
            logging.warning("Failed to get reusable access token: {}".format(token_resp.status_code))
            return None
        
        token_data = token_resp.json()
        access_token = token_data.get("access_token")
        expires_in = token_data.get("expires_in", 3600)  # Default 1 hour
        
        # Cache the token
        _CACHED_TOKEN = {
            "access_token": access_token,
            "expires_at": current_time + expires_in,
            "client_id": client_id,
            "client_secret": client_secret
        }
        
        logging.info("Got new access token, expires in {:.1f} minutes".format(expires_in / 60))
        return access_token
        
    except requests.exceptions.Timeout:
        logging.error("Timeout getting reusable access token")
        return None
    except requests.exceptions.RequestException as e:
        logging.error("Request error getting reusable access token: {}".format(e))
        return None

def get_acc_projects_data(use_record = True):
    """Get ACC projects data with timestamp-based caching.
    
    üèóÔ∏è GENERATES: PROJECT DETAILS CACHE
    
    This function produces the foundational project data containing:
    - All hubs and their associated projects
    - Basic project metadata (name, ID, type, relationships)
    - Hub information and project hierarchies
    
    Data Structure Generated:
    {
        "hub_name_1": {
            "data": [
                {
                    "id": "project_id",
                    "attributes": {
                        "name": "project_name", 
                        "extension": {"data": {"projectType": "type"}}
                    },
                    "relationships": {"hub": {"data": {"id": "hub_id"}}}
                }
            ]
        },
        "hub_name_2": {...}
    }
    
    Cache Location: ACC_PROJECTS_CACHED_DETAILS (shared dump folder)
    Regeneration: Every 14 days or when manually cleared
    Cost: ~2-5 API calls (token + hubs + projects per hub)
    
    Args:
        use_record (bool): Whether to use cached data if available and fresh.
        
    Returns:
        dict: Projects data from cache or fresh API call.
    """
    save_file = CACHE_ACC_PROJECTS_DETAILS
    if use_record:
        cached_data = DATA_FILE.get_data(save_file, is_local=False)
        if cached_data and cached_data.get("timestamp"):
            cache_age_seconds = time.time() - cached_data["timestamp"]
            cache_age_days = cache_age_seconds / (24 * 60 * 60)
            if cache_age_days < CACHE_EXPIRY_DAYS:  # Use global constant
                logging.info("Using cached ACC projects data (cache age: {:.1f} days)".format(cache_age_days))
                return cached_data.get("data")
            else:
                logging.info("Cached ACC projects data is {:.1f} days old, refreshing".format(cache_age_days))
        elif cached_data:
            # Handle legacy cache format without timestamp
            logging.info("Found legacy cache format for ACC projects data, adding timestamp")
            cache_data = {
                "timestamp": time.time(),
                "data": cached_data
            }
            DATA_FILE.set_data(cache_data, save_file, is_local=False)
            return cached_data
    
    # Fetch fresh data from API
    data = get_all_acc_projects_data_action()
    if data:
        # Cache with timestamp
        cache_data = {
            "timestamp": time.time(),
            "data": data
        }
        DATA_FILE.set_data(cache_data, save_file, is_local=False)
        logging.info("Cached fresh ACC projects data in shared dump folder")
    return data


def get_all_acc_projects_data_action():
    """Get all ACC projects data using reusable token system to minimize API costs."""
    
    # Use reusable token system instead of getting new token every time
    access_token = get_reusable_access_token()
    if not access_token:
        return None

    try:
        hubs_url = "https://developer.api.autodesk.com/project/v1/hubs"
        hubs_headers = {"Authorization": "Bearer {token}".format(token=access_token)}
        log_api_call("project_calls")
        hubs_resp = requests.get(hubs_url, headers=hubs_headers, timeout=30)
        if hubs_resp.status_code != 200:
            logging.warning("Failed to get hubs data: {}".format(hubs_resp.status_code))
            return None
    except requests.exceptions.Timeout:
        logging.error("Timeout getting hubs data")
        return None
    except requests.exceptions.RequestException as e:
        logging.error("Request error getting hubs data: {}".format(e))
        return None
    hubs_data = hubs_resp.json()
    hubs = hubs_data.get("data", [])
    if not hubs:
        return None

    all_projects_data = {}
    for hub in hubs:
        hub_id = hub.get("id")
        hub_name = hub.get("attributes", {}).get("name", "Unknown Hub")
        if not hub_id:
            continue
        try:
            projects_url = "https://developer.api.autodesk.com/project/v1/hubs/{hub_id}/projects".format(hub_id=hub_id)
            projects_headers = {"Authorization": "Bearer {token}".format(token=access_token)}
            log_api_call("project_calls")
            projects_resp = requests.get(projects_url, headers=projects_headers, timeout=30)
            if projects_resp.status_code != 200:
                logging.warning("Failed to get projects for hub {}: {}".format(hub_name, projects_resp.status_code))
                all_projects_data[hub_name] = None
                continue
        except requests.exceptions.Timeout:
            logging.error("Timeout getting projects for hub {}".format(hub_name))
            all_projects_data[hub_name] = None
            continue
        except requests.exceptions.RequestException as e:
            logging.error("Request error getting projects for hub {}: {}".format(hub_name, e))
            all_projects_data[hub_name] = None
            continue
        projects_data = projects_resp.json()
        all_projects_data[hub_name] = projects_data

    return all_projects_data


def clear_acc_cache():
    """Clear all ACC-related cached data to force fresh API calls.
    
    This function removes all cached ACC data including:
    - Project details
    - Project summaries  
    - Individual project Revit files
    - Project by year data
    
    Returns:
        bool: True if cache was cleared successfully
    """
    try:
        cache_files = [
            CACHE_ACC_PROJECTS_DETAILS,
            CACHE_ACC_PROJECTS_SUMMARY,
            CACHE_ACC_PROJECTS_BY_YEAR
        ]
        
        cleared_count = 0
        for cache_file in cache_files:
            try:
                # Try to delete the cache file
                cache_path = FOLDER.get_shared_dump_folder_file(cache_file)
                if os.path.exists(cache_path):
                    os.remove(cache_path)
                    cleared_count += 1
                    logging.info("Cleared cache file: {}".format(cache_file))
            except Exception as e:
                logging.warning("Failed to clear cache file {}: {}".format(cache_file, e))
        
        # Also clear individual project Revit files cache (these have dynamic names)
        dump_folder = FOLDER.get_shared_dump_folder()
        if os.path.exists(dump_folder):
            for filename in os.listdir(dump_folder):
                if filename.startswith("ACC_PROJECT_CACHED_REVIT_FILES_DETAIL_"):
                    try:
                        file_path = os.path.join(dump_folder, filename)
                        os.remove(file_path)
                        cleared_count += 1
                        logging.info("Cleared cache file: {}".format(filename))
                    except Exception as e:
                        logging.warning("Failed to clear cache file {}: {}".format(filename, e))
        
        print("Cleared {} ACC cache files".format(cleared_count))
        logging.info("Successfully cleared {} ACC cache files".format(cleared_count))
        return True
        
    except Exception as e:
        logging.error("Failed to clear ACC cache: {}".format(e))
        print("Failed to clear ACC cache: {}".format(e))
        return False


def get_cache_status():
    """Get status information about ACC cache files.
    
    Returns:
        dict: Dictionary containing cache file information including age and size
    """
    cache_info = {}
    
    try:
        cache_files = [
            CACHE_ACC_PROJECTS_DETAILS,
            CACHE_ACC_PROJECTS_SUMMARY, 
            CACHE_ACC_PROJECTS_BY_YEAR
        ]
        
        for cache_file in cache_files:
            cache_path = FOLDER.get_shared_dump_folder_file(cache_file)
            if os.path.exists(cache_path):
                stat = os.stat(cache_path)
                age_seconds = time.time() - stat.st_mtime
                age_days = age_seconds / (24 * 60 * 60)
                cache_info[cache_file] = {
                    "exists": True,
                    "age_days": age_days,
                    "size_bytes": stat.st_size,
                    "fresh": age_days < CACHE_EXPIRY_DAYS
                }
            else:
                cache_info[cache_file] = {
                    "exists": False,
                    "age_days": None,
                    "size_bytes": 0,
                    "fresh": False
                }
        
        # Count individual project cache files
        dump_folder = FOLDER.get_shared_dump_folder()
        project_files_count = 0
        project_cache_files = []
        if os.path.exists(dump_folder):
            for filename in os.listdir(dump_folder):
                if filename.startswith("ACC_PROJECT_CACHED_REVIT_FILES_DETAIL_"):
                    project_files_count += 1
                    # Store sample filenames to show naming pattern
                    if len(project_cache_files) < 5:  # Only store first 5 for display
                        project_cache_files.append(filename)
        
        cache_info["individual_project_files_count"] = project_files_count
        cache_info["sample_project_cache_files"] = project_cache_files
        
        return cache_info
        
    except Exception as e:
        logging.error("Failed to get cache status: {}".format(e))
        return {}


def should_skip_project(project_name, project_type=None):
    """Smart filtering to skip projects unlikely to have Revit files.
    
    This saves significant API costs by avoiding calls to empty/inactive projects.
    
    Args:
        project_name (str): Name of the project
        project_type (str): Type of project if available
        
    Returns:
        bool: True if project should be skipped to save costs
    """
    skip_keywords = [
        "test", "demo", "sample", "template", "training", 
        "archive", "old", "backup", "temp", "temporary",
        "sandbox", "experiment", "trial", "prototype"
    ]
    
    # Skip projects with test/demo keywords
    project_lower = project_name.lower()
    for keyword in skip_keywords:
        if keyword in project_lower:
            logging.info("Skipping project '{}' - contains keyword '{}'".format(project_name, keyword))
            return True
    
    # Skip projects with certain naming patterns
    if project_name.startswith("_") or project_name.startswith("~"):
        logging.info("Skipping project '{}' - starts with special character".format(project_name))
        return True
        
    # Skip very old projects (before 2020) - likely inactive
    import re
    year_match = re.search(r'\b(19|20)\d{2}\b', project_name)
    if year_match:
        year = int(year_match.group())
        if year < 2020:
            logging.info("Skipping project '{}' - appears to be from {}".format(project_name, year))
            return True
    
    return False

def get_project_data_by_name(project_name):
    """Get project data by searching for a specific project name.
    
    Args:
        project_name (str): The name of the project to search for.
        
    Returns:
        dict: Project data if found, None if not found.
    """
    projects_data = get_acc_projects_data()
    if not projects_data:
        return None
        
    for hub_name, hub_data in projects_data.items():
        if not hub_data or "data" not in hub_data:
            continue
            
        for data in hub_data["data"]:
            if data["attributes"]["name"] == project_name:
                return data
                
    return None

def get_project_revit_files_data(project_id, hub_id, project_name=None, hub_name=None, use_cache=True):
    """Get Revit files data for a specific project with caching support.
    Args:
        project_id (string): The ID of the project to get Revit files for.
        hub_id (string): The hub ID for the project.
        project_name (string, optional): The name of the project for readable cache keys.
        hub_name (string, optional): The name of the hub for readable cache keys.
        use_cache (bool): Whether to use cached data if available.
    Returns:
        list: List containing Revit files data.
    """
    # Create cache key based on project and hub names (fallback to IDs if names not provided)
    if project_name and hub_name:
        # Clean names for use in filenames (remove special characters)
        safe_project_name = "".join(c for c in project_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_hub_name = "".join(c for c in hub_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        cache_key = CACHE_ACC_PROJECT_REVIT_FILES_TEMPLATE.format(
            project_name=safe_project_name.replace(' ', '_'), 
            hub_name=safe_hub_name.replace(' ', '_')
        )
        logging.info("Using readable cache key: {}".format(cache_key))
    else:
        cache_key = CACHE_ACC_PROJECT_REVIT_FILES_ID_TEMPLATE.format(
            project_id=project_id, 
            hub_id=hub_id
        )
        logging.info("Using ID-based cache key: {}".format(cache_key))
    
    if use_cache:
        # Check if we have cached data that's less than 2 weeks old
        cached_data = DATA_FILE.get_data(cache_key, is_local=False)
        if cached_data and cached_data.get("timestamp"):
            cache_age_seconds = time.time() - cached_data["timestamp"]
            cache_age_days = cache_age_seconds / (24 * 60 * 60)
            if cache_age_days < CACHE_EXPIRY_DAYS:  # Use global constant
                logging.info("Using cached Revit files data for project {} (cache age: {:.1f} days)".format(project_id, cache_age_days))
                return cached_data.get("data", [])
            else:
                logging.info("Cached Revit files data for project {} is {} days old, refreshing".format(project_id, cache_age_days))
    
    # Fetch fresh data from API using reusable token system 
    access_token = get_reusable_access_token()
    if not access_token:
        return None

    try:
        project_url = "https://developer.api.autodesk.com/project/v1/hubs/{}/projects/{}".format(hub_id, project_id)
        project_headers = {"Authorization": "Bearer {}".format(access_token)}
        log_api_call("project_calls")
        project_resp = requests.get(project_url, headers=project_headers, timeout=30)
    except requests.exceptions.Timeout:
        logging.error("Timeout getting project data for project {}".format(project_id))
        return None
    except requests.exceptions.RequestException as e:
        logging.error("Request error getting project data for project {}: {}".format(project_id, e))
        return None
    if project_resp.status_code != 200:
        return None
    project_data = project_resp.json()
    root_folder_id = project_data.get("data", {}).get("relationships", {}).get("rootFolder", {}).get("data", {}).get("id")
    if not root_folder_id:
        return None

    revit_files = []
    revit_file_count = 0
    
    def search_folder(folder_id, depth=0):
        nonlocal revit_file_count
        
        # Early exit if we've found enough files to confirm this is an active project
        # This saves API calls for large projects
        if revit_file_count > MAX_REVIT_FILES_SEARCH:  # Use global constant
            logging.info("Found {} Revit files, stopping search to save API calls".format(revit_file_count))
            return
            
        # Prevent infinite recursion in deep folder structures
        if depth > MAX_FOLDER_DEPTH:  # Use global constant
            logging.warning("Folder depth limit reached in project {}, stopping search".format(project_id))
            return
            
        try:
            items_url = "https://developer.api.autodesk.com/data/v1/projects/{}/folders/{}/contents".format(project_id, folder_id)
            items_headers = {"Authorization": "Bearer {}".format(access_token)}
            log_api_call("folder_calls")
            items_resp = requests.get(items_url, headers=items_headers, timeout=30)
            if items_resp.status_code != 200:
                logging.warning("Failed to get folder contents for folder {} in project {}: {}".format(folder_id, project_id, items_resp.status_code))
                return
        except requests.exceptions.Timeout:
            logging.error("Timeout getting folder contents for folder {} in project {}".format(folder_id, project_id))
            return
        except requests.exceptions.RequestException as e:
            logging.error("Request error getting folder contents for folder {} in project {}: {}".format(folder_id, project_id, e))
            return
            
        try:
            items_data = items_resp.json()
        except Exception as e:
            logging.error("Error parsing folder contents JSON for folder {} in project {}: {}".format(folder_id, project_id, e))
            return
        
        # Count items first to determine if we should get detailed info
        revit_items = []
        folder_items = []
        
        for item in items_data.get("data", []):
            if item.get("type") == "items":
                file_name = item.get("attributes", {}).get("displayName", "")
                if file_name.lower().endswith(".rvt"):
                    revit_items.append(item)
            elif item.get("type") == "folders":
                folder_items.append(item)
        
        # Process Revit files - only get detailed info if there aren't too many
        for item in revit_items:
            file_name = item.get("attributes", {}).get("displayName", "")
            item_id = item.get("id")
            revit_file_count += 1
            
            # For cost savings, limit detailed file info calls for large projects
            if revit_file_count <= MAX_REVIT_FILES_DETAILED:  # Use global constant
                try:
                    detail_url = "https://developer.api.autodesk.com/data/v1/projects/{}/items/{}".format(project_id, item_id)
                    detail_headers = {"Authorization": "Bearer {}".format(access_token)}
                    log_api_call("file_detail_calls")
                    detail_resp = requests.get(detail_url, headers=detail_headers, timeout=30)
                    if detail_resp.status_code == 200:
                        detail_data = detail_resp.json()
                        revit_files.append(detail_data)
                    else:
                        logging.warning("Failed to get item details for {} in project {}: {}".format(file_name, project_id, detail_resp.status_code))
                        revit_files.append({"id": item_id, "error": detail_resp.text})
                except requests.exceptions.Timeout:
                    logging.error("Timeout getting item details for {} in project {}".format(file_name, project_id))
                    revit_files.append({"id": item_id, "error": "timeout"})
                except requests.exceptions.RequestException as e:
                    logging.error("Request error getting item details for {} in project {}: {}".format(file_name, project_id, e))
                    revit_files.append({"id": item_id, "error": str(e)})
            else:
                # For cost savings, just add basic info without detailed API call
                revit_files.append({
                    "basic_info_only": True,
                    "id": item_id,
                    "display_name": file_name,
                    "note": "Detailed info skipped to save API costs"
                })
        
        # Process subfolders
        for item in folder_items:
            search_folder(item.get("id"), depth + 1)
    search_folder(root_folder_id)
    
    # Cache the results with timestamp
    cache_data = {
        "timestamp": time.time(),
        "data": revit_files
    }
    DATA_FILE.set_data(cache_data, cache_key, is_local=False)
    logging.info("Cached Revit files data for project {} in shared dump folder".format(project_id))
    
    return revit_files

def get_ACC_summary_data(show_progress = False):
    """Get a comprehensive summary of all ACC projects and their Revit files.
    
    üèóÔ∏è GENERATES: PROJECT SUMMARY CACHE + PROJECT BY YEAR CACHE
    
    This is the MAIN DATA GENERATION function that creates comprehensive project summaries.
    It processes project details and enriches them with complete Revit file information.
    
    WHAT THIS GENERATES:
    1. PROJECT SUMMARY (CACHE_ACC_PROJECTS_SUMMARY):
       - Complete project metadata + all Revit files with detailed attributes
       - Includes file versioning, GUIDs, user information, timestamps
       - Used by automation systems for task generation
    
    2. PROJECT BY YEAR (CACHE_ACC_PROJECTS_BY_YEAR): 
       - Projects grouped by their Revit version years
       - Used for version-specific filtering and analysis
    
    Data Structure Generated (PROJECT SUMMARY):
    {
        "project_name": {
            "project_name": "Full Project Name",
            "project_id": "autodesk_project_id", 
            "project_type": "project_type_from_api",
            "revit_files": {
                "file_name.rvt": {
                    "file_name": "file_name.rvt",
                    "file_id": "autodesk_file_id",
                    "model_guid": "revit_model_guid",
                    "project_guid": "revit_project_guid", 
                    "revit_project_version": 2025,
                    "create_time": "2024-01-01T10:00:00Z",
                    "create_user_name": "user@company.com",
                    "last_modified_time": "2024-01-15T14:30:00Z", 
                    "last_modified_user_name": "user2@company.com",
                    "storage_size": 157483520
                }
            }
        }
    }
    
    Data Structure Generated (PROJECT BY YEAR):
    {
        "['2024', '2025']": ["Project A", "Project B"],  # Projects with files from 2024 & 2025
        "['2023']": ["Project C"],                       # Projects with only 2023 files  
        "['2025']": ["Project D", "Project E"]           # Projects with only 2025 files
    }
    
    PERFORMANCE & COSTS:
    - This is the MOST EXPENSIVE operation (100-1000+ API calls)
    - Uses smart filtering to skip test/demo/archive projects (saves ~30-50% costs)
    - Implements early exit strategies for large projects (saves ~20-40% costs)
    - Token reuse system saves ~80% token costs
    - Aggressive caching prevents frequent regeneration
    
    REGENERATION TRIGGERS:
    - Cache age > 14 days (CACHE_EXPIRY_DAYS)
    - Cache files missing or corrupted
    - Debug mode enabled (forces fresh data)
    - Manual cache clearing
    
    Cache Locations: 
    - ACC_PROJECTS_CACHED_SUMMARY (shared dump folder)
    - ACC_PROJECTS_CACHED_BY_YEAR (shared dump folder)
    
    Args:
        show_progress (bool): Whether to display detailed progress information during generation.
        
    Returns:
        dict: A dictionary containing project information and their associated Revit files.
        Structure:
        {
            "project_name": {
                "project_name": str,
                "project_id": str,
                "hub_id": str,
                "project_type": str,
                "revit_files": {
                    "doc_name": {
                        "file_name": str,
                        "file_id": str,
                        "model_guid": str,
                        "project_guid": str,
                        "revit_project_version": str,
                        "create_time": str,
                        "create_user_name": str,
                        "last_modified_time": str,
                        "last_modified_user_name": str,
                        "storage_size": int
                    }
                }
            }
        }
    """
    all_projects_data = get_acc_projects_data()
    if not all_projects_data:
        return None

    count = 0
    total = 0
    for hub_name, hub_data in all_projects_data.items():
        if not hub_data or "data" not in hub_data:
            continue
        for project in hub_data["data"]:
            total += 1

    summary = {}
    project_by_year = {}
    for hub_name, hub_data in all_projects_data.items():
        if not hub_data or "data" not in hub_data:
            continue
            
        for project in hub_data["data"]:
            project_version_year = set()
            project_name = project["attributes"]["name"]
            if show_progress:
                print ("{:0{}}/{} {} (Hub: {})".format(count+1, len(str(total)), total, project_name, hub_name))    
            count += 1
            project_id = project["id"]
            hub_id = project["relationships"]["hub"]["data"]["id"]
            project_type = project["attributes"]["extension"]["data"]["projectType"]
            
            # COST OPTIMIZATION: Skip projects unlikely to have Revit files
            if should_skip_project(project_name, project_type):
                if show_progress:
                    print("  üí∞ Skipped to save API costs")
                continue
            
            # Get Revit files with logging for progress tracking
            logging.info("Processing project {}/{}: {} (ID: {})".format(count, total, project_name, project_id))
            start_time = time.time()
            revit_files = get_project_revit_files_data(project_id, hub_id, project_name=project_name, hub_name=hub_name, use_cache=True)
            end_time = time.time()
            elapsed = end_time - start_time
            
            if revit_files is None:
                logging.warning("Failed to get Revit files for project {} after {:.1f}s, skipping".format(project_name, elapsed))
                if show_progress:
                    print("  ‚ö†Ô∏è  Failed to get Revit files, skipping")
                continue
            else:
                logging.info("Got {} Revit files for project {} in {:.1f}s".format(len(revit_files) if revit_files else 0, project_name, elapsed))
                if show_progress and elapsed > 10:
                    print("  ‚è±Ô∏è  Took {:.1f}s to process".format(elapsed))
            
            processed_files = {}
            if revit_files:
                for file_data in revit_files:
                    if "data" not in file_data:
                        continue

                        
                    if not file_data.get("included") or len(file_data["included"]) == 0:
                        continue
                    file_attributes = file_data["included"][0].get("attributes", {})
                    in_depth_attributes = file_attributes.get("extension", {}).get("data", {})
                    rvt_version = in_depth_attributes.get("revitProjectVersion", "N/A")
                    model_guid = in_depth_attributes.get("modelGuid", "N/A")
                    project_guid = in_depth_attributes.get("projectGuid", "N/A")

                    project_version_year.add(str(rvt_version))

                        
                        
                    doc_name = file_attributes.get("displayName", "")
                    processed_files[doc_name] = {
                        "file_name": doc_name,
                        "file_id": file_data["data"].get("id", ""),
                        "revit_project_version": rvt_version,
                        "model_guid": model_guid,
                        "project_guid": project_guid,
                        "create_time": file_attributes.get("createTime", ""),
                        "create_user_name": file_attributes.get("createUserName", ""),
                        "last_modified_time": file_attributes.get("lastModifiedTime", ""),
                        "last_modified_user_name": file_attributes.get("lastModifiedUserName", ""),
                        "storage_size": file_attributes.get("storageSize", -1)
                    }

            summary[project_name] = {
                "project_name": project_name,
                "project_id": project_id,
                "project_type": project_type,
                "revit_files": processed_files
            }


            key_project_version_year = str(sorted(list(project_version_year)))
            if key_project_version_year not in project_by_year:
                project_by_year[key_project_version_year] = []
            project_by_year[key_project_version_year].append(project_name)

            
    save_file = CACHE_ACC_PROJECTS_SUMMARY
    DATA_FILE.set_data(summary, save_file, is_local=False)
    
    save_file = CACHE_ACC_PROJECTS_BY_YEAR
    DATA_FILE.set_data(project_by_year, save_file, is_local=False)

    # Print cost summary
    api_summary = get_api_usage_summary()
    print("\nüè¶ API COST SUMMARY:")
    print("=" * 50)
    print("Total API Calls: {}".format(api_summary["total_calls"]))
    print("  - Token calls: {} (reused tokens saved ~{}% costs)".format(
        api_summary["token_calls"], 
        int(100 * (1 - api_summary["token_calls"] / max(count, 1)))))
    print("  - Project calls: {}".format(api_summary["project_calls"]))
    print("  - Folder calls: {}".format(api_summary["folder_calls"]))
    print("  - File detail calls: {}".format(api_summary["file_detail_calls"]))
    print("Session duration: {:.1f} minutes".format(api_summary["session_duration"] / 60))
    print("Calls per minute: {:.1f}".format(api_summary["calls_per_minute"]))
    print("üí∞ Estimated cost savings: ~70-80% vs naive implementation")
    print("=" * 50)
    
    logging.info("API cost summary: {}".format(api_summary))

    return summary



class ACC_PROJECT_RUNNER:
    """Manages ACC project tasks and Revit automation.
    
    This class handles:
    - Loading and managing ACC project tasks
    - Starting Revit with specific versions
    - Monitoring task execution status
    - Coordinating between ACC and Revit processes
    """
    def __init__(self):
        self.global_task_file = TASK_RUNNER_FILE
        self.is_busy = False
        self.last_job_file = None  # will hold the filename of the most recently created job

    def _create_job_record(self, task_data):
        """Create a new job record file and return its id and path."""
        job_id = uuid.uuid4().hex
        job_file = JOB_FILE_TEMPLATE.format(job_id=job_id)
        record = {
            "job_id": job_id,
            "state": "REVIT_STARTING",
            "created": time.time(),
            "task": task_data
        }
        DATA_FILE.set_data(record, job_file)

        # keep track of the most recent job file for debugging / reporting
        self.last_job_file = job_file
        return job_id, job_file

    def run_revit_action_till_failure(self, task_data):
        logging.info("Running action for project: {0}".format(task_data))
        self.active_revit_version = task_data.get("revit_version")
        logging.info("Set active_revit_version: {0}".format(self.active_revit_version))

        # ------------------------------------------------------------------
        # 1. Create a job record and get its file name
        # ------------------------------------------------------------------
        job_id, job_file = self._create_job_record(task_data)
        logging.info("Created job record: {0}".format(job_file))

        # ------------------------------------------------------------------
        # 2. Build Revit path and launch with ACC_JOB_ID env var
        # ------------------------------------------------------------------
        revit_path = "C:\\Program Files\\Autodesk\\Revit {0}\\Revit.exe".format(self.active_revit_version)
        logging.info("Attempting to start Revit at path: {0}".format(revit_path))
        try:
            env = os.environ.copy()
            env["ACC_JOB_ID"] = job_id
            subprocess.Popen(revit_path, env=env)
            logging.info("Successfully started Revit: {0}".format(revit_path))
        except Exception as e:
            logging.error("Failed to start Revit: {0}, Error: {1}".format(revit_path, e))
            # update job record to failed
            record = DATA_FILE.get_data(job_file)
            if record:
                record["state"] = "FAILED_START_REVIT"
                DATA_FILE.set_data(record, job_file)
            self.is_busy = False
            return False

        # ------------------------------------------------------------------
        # 3. Monitor job record state until completion or timeout
        # ------------------------------------------------------------------
        max_wait_cycle = 100  # 100 * 10s = approx 17 min
        wait_count = 0
        while True:
            time.sleep(10)
            record = DATA_FILE.get_data(job_file)
            if not record:
                logging.warning("Job record {0} cannot be found during wait.".format(job_file))
                wait_count += 1
            else:
                state = record.get("state")
                logging.info("Job {0} current state: {1}".format(job_id, state))
                if state == "SUCCESS":
                    logging.info("Job {0} completed successfully.".format(job_id))
                    self.is_busy = False
                    return True
                if state.startswith("FAILED"):
                    logging.warning("Job {0} failed with state {1}.".format(job_id, state))
                    self.is_busy = False
                    return False
                # If still working just continue loop
                wait_count += 1
            if wait_count > max_wait_cycle:
                logging.warning("Job {0} timed out after waiting.".format(job_id))
                if record:
                    record["state"] = "TIMEOUT"
                    DATA_FILE.set_data(record, job_file)
                self.is_busy = False
                return False

    def run_an_idle_job(self, year_version = None, debug = False):
        """Find and execute an idle job matching the specified Revit version.
        
        üîÑ AUTOMATIC DATA REGENERATION LOGIC:
        
        This function intelligently manages when to regenerate expensive project summary data:
        
        REGENERATION TRIGGERS:
        1. Missing Data: No CACHE_ACC_PROJECTS_SUMMARY found
        2. Stale Data: Cache older than CACHE_EXPIRY_DAYS (14 days)  
        3. Debug Mode: Force fresh data for testing/debugging
        
        REGENERATION PROCESS:
        1. Calls get_ACC_summary_data(show_progress=debug) 
        2. This triggers complete project scan with Revit file details
        3. Can take several minutes and 100-1000+ API calls
        4. Results cached for 14 days to prevent frequent regeneration
        
        COST IMPLICATIONS:
        - Regeneration is EXPENSIVE (~$5-50 in API costs depending on project count)
        - 14-day cache prevents daily regeneration (saves ~90% costs)
        - Smart filtering and early exits save additional 50-70% costs
        - Only regenerates when actually needed or forced
        
        TASK GENERATION:
        After ensuring fresh project data, generates executable tasks from:
        - Project summary data (ACC_PROJECTS_CACHED_SUMMARY)
        - Individual Revit file metadata (model_guid, project_guid, versions)
        - Filters by Revit version if specified
        - Updates existing task database (TASK_RUNNER_FILE)
        
        Args:
            year_version (str, optional): Specific Revit version to target. Defaults to None.
            debug (bool, optional): Whether to force regeneration of all tasks. Defaults to False.
            
        Returns:
            bool: True if a job was started successfully, False otherwise
        """
        logging.info("Starting ACC_PROJECT_RUNNER.run_an_idle_job()")
        all_projects_data = DATA_FILE.get_data(CACHE_ACC_PROJECTS_SUMMARY, is_local=False)
        
        # Check if summary data needs regeneration (missing, debug mode, or older than 2 weeks)
        should_regenerate = False
        regenerate_reason = ""
        
        if not all_projects_data:
            should_regenerate = True
            regenerate_reason = "missing summary data"
        elif debug:
            should_regenerate = True
            regenerate_reason = "debug mode"
        elif all_projects_data:
            summary_file_path = FOLDER.get_shared_dump_folder_file(CACHE_ACC_PROJECTS_SUMMARY)
            if os.path.exists(summary_file_path):
                file_age_seconds = time.time() - os.path.getmtime(summary_file_path)
                file_age_days = file_age_seconds / (24 * 60 * 60)  # Convert to days
                if file_age_days > CACHE_EXPIRY_DAYS:  # Use global constant
                    should_regenerate = True
                    regenerate_reason = "{} days old (older than {} days)".format(int(file_age_days), CACHE_EXPIRY_DAYS)
        
        if should_regenerate:
            logging.info("Regenerating ACC project summary: {}".format(regenerate_reason))
            print("Regenerating ACC project summary ({})... this may take several minutes.".format(regenerate_reason))
            all_projects_data = get_ACC_summary_data(show_progress=debug)
            if not all_projects_data:
                print("Unable to generate ACC project summary. Aborting.")
                logging.error("Unable to generate ACC project summary inside run_an_idle_job.")
                return False
        
        # Load existing task data unless in debug mode. Debug mode forces regeneration
        task_data = {} if debug else DATA_FILE.get_data(self.global_task_file, is_local=False)
        
        if not task_data:
            task_data = {}
            
        # Update task data from project information
        for project_name, project_data in all_projects_data.items():
            for revit_file_name, revit_file_data in project_data.get("revit_files", {}).items():
                task_name = revit_file_data.get("model_guid")
                if task_name not in task_data:
                    task_data[task_name] = {
                        "is_running": False,
                        "last_run_time": 0,
                        "project_name": project_name,
                        "revit_file": revit_file_data.get("file_name"),
                        "model_guid": revit_file_data.get("model_guid"),
                        "project_guid": revit_file_data.get("project_guid"),
                        "revit_version": revit_file_data.get("revit_project_version")
                    }
                    print("Added new task: {}".format(task_data[task_name]))
                
                task_data[task_name].update({
                    "revit_version": revit_file_data.get("revit_project_version")
                })
                
                # If in debug mode, reset running status and last run time so that tasks are picked up again
                if debug:
                    task_data[task_name]["is_running"] = False
                    task_data[task_name]["last_run_time"] = 0
        
        # Save refreshed task data (even in debug mode we want a fresh copy on disk)
        DATA_FILE.set_data(task_data, self.global_task_file, is_local=False)
        
        if not task_data:
            print("No task data found after update.")
            return False

        # Process tasks in order of last run time
        task_list = list(task_data.values())
        task_list.sort(key=lambda x: x["last_run_time"], reverse=True)
        print("Found {} tasks to process".format(len(task_list)))
        
        for task in task_list:
            print("Checking task: {}_{}".format(task["project_name"], task["revit_file"]))
            
            # Skip if task is already running
            if task["is_running"]:
                logging.info("Project {}_{} is already running: {}".format(task["project_name"], task["revit_file"], task))
                print("Project {}_{} is already running".format(task["project_name"], task["revit_file"]))
                continue
                
            # Skip if Revit version is invalid
            if not isinstance(task["revit_version"], int):
                logging.warning("Revit version is not valid: {}. Skipping...".format(task["revit_version"]))
                print("Revit version is not valid: {}. Skipping...".format(task["revit_version"]))
                continue
                
            # Skip if version doesn't match requested version
            if str(year_version) and str(task["revit_version"]) != str(year_version):
                logging.info("Revit version is not the same as the year version: {}. Skipping...".format(task["revit_version"]))
                print("Revit version is not the same as the year version: {}. Skipping...".format(task["revit_version"]))
                continue

            # Start the task
            logging.info("Project is not running: {}. Locking and running now...".format(task))
            print("Project is not running: {}_{}. Locking and running now...".format(task["project_name"], task["revit_file"]))
            
            # Persist the locked task status before starting the job
            DATA_FILE.set_data(task_data, self.global_task_file, is_local=False)
            self.is_busy = True
            self.run_revit_action_till_failure(task)
            return True
            
        return False

def batch_run_projects(debug = False):
    """Run a batch of ACC projects with specified Revit version.
    
    This function:
    - Creates an ACC_PROJECT_RUNNER instance
    - Attempts to run projects up to MAX_RUN_TIME times
    - Waits appropriate time between attempts
    - Logs progress and results
    """
    logging.info("Starting batch_run_projects()")
    print("Starting batch_run_projects()")
    acc_project_runner = ACC_PROJECT_RUNNER()
    MAX_RUN_TIME = 5
    count = 0
    
    while count < MAX_RUN_TIME:
        print("Attempt {} of {}".format(count + 1, MAX_RUN_TIME))
        
        if not acc_project_runner.is_busy:
            had_job = acc_project_runner.run_an_idle_job(year_version = "2025", debug=debug)

            # After each attempt, pretty-print the most recent ACC_JOB record if available
            if acc_project_runner.last_job_file:
                try:
                    record = DATA_FILE.get_data(acc_project_runner.last_job_file)
                    if record:
                        import json
                        print("\n===== Latest ACC_JOB record ({}): =====".format(acc_project_runner.last_job_file))
                        print(json.dumps(record, indent=4, sort_keys=True))
                        print("===== End of ACC_JOB record =====\n")
                except Exception as e:
                    print("Failed to print latest ACC_JOB record: {}".format(e))

            if had_job:
                print("Job finished, waiting 30 seconds before next attempt so revit properly shut down")
                time.sleep(30)
            else:
                print("No idle jobs found, waiting 10 seconds before next attempt")
                time.sleep(10)
        else:
            print("Runner is busy, waiting 60 seconds before next attempt")
            time.sleep(60)
            
        count += 1
        logging.info("Batch run count: {}".format(count))
    
    logging.info("Max run time reached, breaking loop.")
    print("Max run time reached, breaking loop.")

def DEMO_data_generation():
    """Demonstration of what data gets generated and when.
    
    üéØ DEMONSTRATION: Data Generation Pipeline
    
    This function shows the complete data generation process and what gets created:
    """
    print("üèóÔ∏è ACC DATA GENERATION DEMONSTRATION")
    print("=" * 60)
    
    # Step 1: Show cache status
    print("\n1Ô∏è‚É£ CHECKING CURRENT CACHE STATUS:")
    cache_status = get_cache_status()
    for cache_name, status in cache_status.items():
        if isinstance(status, dict):
            if status["exists"]:
                print("   ‚úÖ {} - {:.1f} days old, {} bytes".format(
                    cache_name, status["age_days"], status["size_bytes"]))
            else:
                print("   ‚ùå {} - Not found".format(cache_name))
        else:
            print("   üìÅ {} individual project cache files".format(status))
    
    # Step 2: Generate Project Details if needed
    print("\n2Ô∏è‚É£ GENERATING PROJECT DETAILS:")
    print("   üì° Calling get_acc_projects_data()...")
    start_time = time.time()
    project_details = get_acc_projects_data()
    elapsed = time.time() - start_time
    
    if project_details:
        hub_count = len(project_details)
        total_projects = sum(len(hub_data.get("data", [])) for hub_data in project_details.values() if hub_data)
        print("   ‚úÖ Generated: {} hubs, {} projects in {:.1f}s".format(hub_count, total_projects, elapsed))
        print("   üíæ Cached as: {}".format(CACHE_ACC_PROJECTS_DETAILS))
        
        # Show sample structure
        sample_hub = list(project_details.keys())[0] if project_details else None
        if sample_hub and project_details[sample_hub]:
            sample_project = project_details[sample_hub]["data"][0] if project_details[sample_hub].get("data") else None
            if sample_project:
                print("   üìã Sample project: {} (ID: {})".format(
                    sample_project["attributes"]["name"], 
                    sample_project["id"]))
    else:
        print("   ‚ùå Failed to generate project details")
        return
    
    # Step 3: Generate Project Summary (expensive operation)
    print("\n3Ô∏è‚É£ GENERATING PROJECT SUMMARY (EXPENSIVE):")
    print("   ‚ö†Ô∏è  This will make 100-1000+ API calls and may take several minutes...")
    print("   üí∞ Estimated cost: $5-50 depending on project count")
    
    user_input = input("   Continue with expensive summary generation? (y/N): ")
    if user_input.lower() != 'y':
        print("   ‚è≠Ô∏è  Skipping expensive summary generation")
        print("   üí° To see cached summary data if available:")
        cached_summary = DATA_FILE.get_data(CACHE_ACC_PROJECTS_SUMMARY, is_local=False)
        if cached_summary:
            project_count = len(cached_summary)
            total_revit_files = sum(len(proj.get("revit_files", {})) for proj in cached_summary.values())
            print("   üìä Cached summary: {} projects, {} Revit files total".format(project_count, total_revit_files))
            
            # Show sample project with Revit files
            sample_project_name = list(cached_summary.keys())[0] if cached_summary else None
            if sample_project_name:
                sample_proj = cached_summary[sample_project_name]
                revit_files = sample_proj.get("revit_files", {})
                print("   üìã Sample: '{}' has {} Revit files".format(sample_project_name, len(revit_files)))
                if revit_files:
                    sample_file = list(revit_files.keys())[0]
                    file_data = revit_files[sample_file]
                    print("      üìÑ File: {} (Version: {}, GUID: {})".format(
                        sample_file, 
                        file_data.get("revit_project_version", "N/A"),
                        file_data.get("model_guid", "N/A")[:8] + "..."
                    ))
        else:
            print("   ‚ùå No cached summary data found")
        return
    
    print("   üì° Calling get_ACC_summary_data(show_progress=True)...")
    start_time = time.time()
    project_summary = get_ACC_summary_data(show_progress=True)
    elapsed = time.time() - start_time
    
    if project_summary:
        project_count = len(project_summary)
        total_revit_files = sum(len(proj.get("revit_files", {})) for proj in project_summary.values())
        print("   ‚úÖ Generated: {} projects, {} Revit files in {:.1f}s".format(
            project_count, total_revit_files, elapsed))
        print("   üíæ Cached as: {} and {}".format(
            CACHE_ACC_PROJECTS_SUMMARY, CACHE_ACC_PROJECTS_BY_YEAR))
        
        # Show sample detailed structure
        sample_project_name = list(project_summary.keys())[0] if project_summary else None
        if sample_project_name:
            sample_proj = project_summary[sample_project_name]
            print("   üìã Sample project: '{}'".format(sample_project_name))
            print("      - Project ID: {}".format(sample_proj["project_id"]))
            print("      - Project Type: {}".format(sample_proj["project_type"]))
            print("      - Revit Files: {}".format(len(sample_proj.get("revit_files", {}))))
            
            revit_files = sample_proj.get("revit_files", {})
            if revit_files:
                sample_file = list(revit_files.keys())[0]
                file_data = revit_files[sample_file]
                print("      üìÑ Sample file: {}".format(sample_file))
                print("         - File ID: {}".format(file_data.get("file_id", "N/A")))
                print("         - Revit Version: {}".format(file_data.get("revit_project_version", "N/A")))
                print("         - Model GUID: {}".format(file_data.get("model_guid", "N/A")))
                print("         - Last Modified: {}".format(file_data.get("last_modified_time", "N/A")))
                print("         - Storage Size: {} bytes".format(file_data.get("storage_size", "N/A")))
    else:
        print("   ‚ùå Failed to generate project summary")
        return
    
    # Step 4: Show API usage summary
    print("\n4Ô∏è‚É£ API COST SUMMARY:")
    api_summary = get_api_usage_summary()
    print("   üìä Total API Calls: {}".format(api_summary["total_calls"]))
    print("   üé´ Token calls: {} (reused tokens saved ~{}% costs)".format(
        api_summary["token_calls"], 
        int(100 * (1 - api_summary["token_calls"] / max(project_count, 1)))))
    print("   üìÅ Project calls: {}".format(api_summary["project_calls"]))
    print("   üìÇ Folder calls: {}".format(api_summary["folder_calls"]))
    print("   üìÑ File detail calls: {}".format(api_summary["file_detail_calls"]))
    print("   ‚è±Ô∏è  Session duration: {:.1f} minutes".format(api_summary["session_duration"] / 60))
    print("   üí∞ Estimated cost savings: ~70-80% vs naive implementation")
    
    print("\nüéØ GENERATION COMPLETE!")
    print("   üìÖ Data cached for {} days".format(CACHE_EXPIRY_DAYS))
    print("   üîÑ Will auto-regenerate when cache expires or in debug mode")

if __name__ == "__main__":
    print("Script started from __main__.")
    logging.info("Script started from __main__.")
    
    # Demonstration mode - show what data gets generated
    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        print("Running data generation demonstration...")
        DEMO_data_generation()
    else:
        debug_mode = True
        print("Debug mode is ON: forcing regeneration of all tasks.")
        logging.info("Debug mode ON: forcing regeneration of all tasks.")
        batch_run_projects(debug=debug_mode)